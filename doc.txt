/cfsdata2/cfsdata/datasets/coco
git教程:
  git init
  git add README.md
  git commit -m "first commit"
  git branch -M main
  git remote add origin https://github.com/YatesZhang/retrieval.git
  git push -u origin main

force merge conflict:
  git reset --hard FETCH_HEAD
  git pull 
  git remote set-url origin https://github.com/YatesZhang/retrieval.git

ps -aux | grep <pid>
tmux后台：
  tmux ls
  tmux new -s fastrcnn 
  tmux attach -t fastrcnn
  tmux kill-session -t fastrcnn


LoRA tuning: 
lora_target_modules=["q_proj", "k_proj", "v_proj", "o_proj",    #  attention layer in LLaMa
                   "to_q", "to_kv", "to_out",    # gate cross layer attention 
                    "ff.1", "ff.3"],    # 
        │   │       ├── gated_cross_attn_layer (GatedCrossAttentionBlock) attn_gate:[1] ff_gate:[1]
        │   │       │   ├── attn (MaskedCrossAttention)
        │   │       │   │   ├── norm (LayerNorm) weight:[4096] bias:[4096]
        │   │       │   │   ├── to_q (Linear) weight:[512, 4096]
        │   │       │   │   │   ├── lora_dropout (ModuleDict)
        │   │       │   │   │   ├── lora_A (ModuleDict)
        │   │       │   │   │   │   └── default (Linear) weight:[16, 4096]
        │   │       │   │   │   └── lora_B (ModuleDict)
        │   │       │   │   │       └── default (Linear) weight:[512, 16]
        │   │       │   │   ├── to_kv (Linear) weight:[1024, 1024]
        │   │       │   │   │   ├── lora_dropout (ModuleDict)
        │   │       │   │   │   ├── lora_A (ModuleDict)
        │   │       │   │   │   │   └── default (Linear) weight:[16, 1024]
        │   │       │   │   │   └── lora_B (ModuleDict)
        │   │       │   │   │       └── default (Linear) weight:[1024, 16]
        │   │       │   │   └── to_out (Linear) weight:[4096, 512]
        │   │       │   │       ├── lora_dropout (ModuleDict)
        │   │       │   │       ├── lora_A (ModuleDict)
        │   │       │   │       │   └── default (Linear) weight:[16, 512]
        │   │       │   │       └── lora_B (ModuleDict)
        │   │       │   │           └── default (Linear) weight:[4096, 16]
        │   │       │   └── ff (Sequential)
        │   │       │       ├── 0 (LayerNorm) weight:[4096] bias:[4096]
        │   │       │       ├── 1 (Linear) weight:[16384, 4096]
        │   │       │       │   ├── lora_dropout (ModuleDict)
        │   │       │       │   ├── lora_A (ModuleDict)
        │   │       │       │   │   └── default (Linear) weight:[16, 4096]
        │   │       │       │   └── lora_B (ModuleDict)
        │   │       │       │       └── default (Linear) weight:[16384, 16]
        │   │       │       └── 3 (Linear) weight:[4096, 16384]
        │   │       │           ├── lora_dropout (ModuleDict)
        │   │       │           ├── lora_A (ModuleDict)
        │   │       │           │   └── default (Linear) weight:[16, 16384]
        │   │       │           └── lora_B (ModuleDict)
        │   │       │               └── default (Linear) weight:[4096, 16]

root
├── model (LlamaModel)
│   ├── embed_tokens (Embedding) weight:[32003, 4096]
│   ├── layers (ModuleList)
│   │   └── 0-31(LlamaDecoderLayer)
│   │       ├── self_attn (LlamaAttention)
│   │       │   └── q_proj,k_proj,v_proj,o_proj(Linear) weight:[4096, 4096]
│   │       ├── mlp (LlamaMLP)
│   │       │   ├── gate_proj,up_proj(Linear) weight:[11008, 4096]
│   │       │   └── down_proj (Linear) weight:[4096, 11008]
│   │       └── input_layernorm,post_attention_layernorm(LlamaRMSNorm) weight:[4096]
│   └── norm (LlamaRMSNorm) weight:[4096]
└── lm_head (Linear) weight:[32003, 4096]

root
└── base_model (LoraModel)
    └── model (LlamaForCausalLM)
        ├── model (LlamaModel)
        │   ├── embed_tokens (Embedding) weight:[32003, 4096]
        │   ├── layers (ModuleList)
        │   │   ├── 0-2,4-6,8-10,12-14,16-18,20-22,24-26,28-30(FlamingoLayer)
        │   │   │   └── decoder_layer (LlamaDecoderLayer)
        │   │   │       ├── self_attn (LlamaAttention)
        │   │   │       │   └── q_proj,k_proj,v_proj,o_proj(Linear) weight:[4096, 4096]
        │   │   │       │       ├── lora_dropout (ModuleDict)
        │   │   │       │       ├── lora_A (ModuleDict)
        │   │   │       │       │   └── default (Linear) weight:[16, 4096]
        │   │   │       │       └── lora_B (ModuleDict)
        │   │   │       │           └── default (Linear) weight:[4096, 16]
        │   │   │       ├── mlp (LlamaMLP)
        │   │   │       │   ├── gate_proj,up_proj(Linear) weight:[11008, 4096]
        │   │   │       │   └── down_proj (Linear) weight:[4096, 11008]
        │   │   │       └── input_layernorm,post_attention_layernorm(LlamaRMSNorm) weight:[4096]
        │   │   └── 3-3,7-7,11-11,15-15,19-19,23-23,27-27,31-31(FlamingoLayer)
        │   │       ├── gated_cross_attn_layer (GatedCrossAttentionBlock) attn_gate:[1] ff_gate:[1]
        │   │       │   ├── attn (MaskedCrossAttention)
        │   │       │   │   ├── norm (LayerNorm) weight:[4096] bias:[4096]
        │   │       │   │   ├── to_q (Linear) weight:[512, 4096]
        │   │       │   │   │   ├── lora_dropout (ModuleDict)
        │   │       │   │   │   ├── lora_A (ModuleDict)
        │   │       │   │   │   │   └── default (Linear) weight:[16, 4096]
        │   │       │   │   │   └── lora_B (ModuleDict)
        │   │       │   │   │       └── default (Linear) weight:[512, 16]
        │   │       │   │   ├── to_kv (Linear) weight:[1024, 1024]
        │   │       │   │   │   ├── lora_dropout (ModuleDict)
        │   │       │   │   │   ├── lora_A (ModuleDict)
        │   │       │   │   │   │   └── default (Linear) weight:[16, 1024]
        │   │       │   │   │   └── lora_B (ModuleDict)
        │   │       │   │   │       └── default (Linear) weight:[1024, 16]
        │   │       │   │   └── to_out (Linear) weight:[4096, 512]
        │   │       │   │       ├── lora_dropout (ModuleDict)
        │   │       │   │       ├── lora_A (ModuleDict)
        │   │       │   │       │   └── default (Linear) weight:[16, 512]
        │   │       │   │       └── lora_B (ModuleDict)
        │   │       │   │           └── default (Linear) weight:[4096, 16]
        │   │       │   └── ff (Sequential)
        │   │       │       ├── 0 (LayerNorm) weight:[4096] bias:[4096]
        │   │       │       ├── 1 (Linear) weight:[16384, 4096]
        │   │       │       │   ├── lora_dropout (ModuleDict)
        │   │       │       │   ├── lora_A (ModuleDict)
        │   │       │       │   │   └── default (Linear) weight:[16, 4096]
        │   │       │       │   └── lora_B (ModuleDict)
        │   │       │       │       └── default (Linear) weight:[16384, 16]
        │   │       │       └── 3 (Linear) weight:[4096, 16384]
        │   │       │           ├── lora_dropout (ModuleDict)
        │   │       │           ├── lora_A (ModuleDict)
        │   │       │           │   └── default (Linear) weight:[16, 16384]
        │   │       │           └── lora_B (ModuleDict)
        │   │       │               └── default (Linear) weight:[4096, 16]
        │   │       └── decoder_layer (LlamaDecoderLayer)
        │   │           ├── self_attn (LlamaAttention)
        │   │           │   └── q_proj,k_proj,v_proj,o_proj(Linear) weight:[4096, 4096]
        │   │           │       ├── lora_dropout (ModuleDict)
        │   │           │       ├── lora_A (ModuleDict)
        │   │           │       │   └── default (Linear) weight:[16, 4096]
        │   │           │       └── lora_B (ModuleDict)
        │   │           │           └── default (Linear) weight:[4096, 16]
        │   │           ├── mlp (LlamaMLP)
        │   │           │   ├── gate_proj,up_proj(Linear) weight:[11008, 4096]
        │   │           │   └── down_proj (Linear) weight:[4096, 11008]
        │   │           └── input_layernorm,post_attention_layernorm(LlamaRMSNorm) weight:[4096]
        │   └── norm (LlamaRMSNorm) weight:[4096]
        ├── lm_head (Linear) weight:[32003, 4096]
        └── gated_cross_attn_layers (ModuleList)
            └── 3-3,7-7,11-11,15-15,19-19,23-23,27-27,31-31(GatedCrossAttentionBlock) attn_gate:[1] ff_gate:[1]
                ├── attn (MaskedCrossAttention)
                │   ├── norm (LayerNorm) weight:[4096] bias:[4096]
                │   ├── to_q (Linear) weight:[512, 4096]
                │   │   ├── lora_dropout (ModuleDict)
                │   │   ├── lora_A (ModuleDict)
                │   │   │   └── default (Linear) weight:[16, 4096]
                │   │   └── lora_B (ModuleDict)
                │   │       └── default (Linear) weight:[512, 16]
                │   ├── to_kv (Linear) weight:[1024, 1024]
                │   │   ├── lora_dropout (ModuleDict)
                │   │   ├── lora_A (ModuleDict)
                │   │   │   └── default (Linear) weight:[16, 1024]
                │   │   └── lora_B (ModuleDict)
                │   │       └── default (Linear) weight:[1024, 16]
                │   └── to_out (Linear) weight:[4096, 512]
                │       ├── lora_dropout (ModuleDict)
                │       ├── lora_A (ModuleDict)
                │       │   └── default (Linear) weight:[16, 512]
                │       └── lora_B (ModuleDict)
                │           └── default (Linear) weight:[4096, 16]
                └── ff (Sequential)
                    ├── 0 (LayerNorm) weight:[4096] bias:[4096]
                    ├── 1 (Linear) weight:[16384, 4096]
                    │   ├── lora_dropout (ModuleDict)
                    │   ├── lora_A (ModuleDict)
                    │   │   └── default (Linear) weight:[16, 4096]
                    │   └── lora_B (ModuleDict)
                    │       └── default (Linear) weight:[16384, 16]
                    └── 3 (Linear) weight:[4096, 16384]
                        ├── lora_dropout (ModuleDict)
                        ├── lora_A (ModuleDict)
                        │   └── default (Linear) weight:[16, 16384]
                        └── lora_B (ModuleDict)
                            └── default (Linear) weight:[4096, 16]



        │   │       ├── gated_cross_attn_layer (GatedCrossAttentionBlock) attn_gate:[1] ff_gate:[1]
        │   │       │   ├── attn (MaskedCrossAttention)
        │   │       │   │   ├── norm (LayerNorm) weight:[4096] bias:[4096]
        │   │       │   │   ├── to_q (Linear) weight:[512, 4096]
        │   │       │   │   │   ├── lora_dropout (ModuleDict)
        │   │       │   │   │   ├── lora_A (ModuleDict)
        │   │       │   │   │   │   └── default (Linear) weight:[16, 4096]
        │   │       │   │   │   └── lora_B (ModuleDict)
        │   │       │   │   │       └── default (Linear) weight:[512, 16]
        │   │       │   │   ├── to_kv (Linear) weight:[1024, 1024]
        │   │       │   │   │   ├── lora_dropout (ModuleDict)
        │   │       │   │   │   ├── lora_A (ModuleDict)
        │   │       │   │   │   │   └── default (Linear) weight:[16, 1024]
        │   │       │   │   │   └── lora_B (ModuleDict)
        │   │       │   │   │       └── default (Linear) weight:[1024, 16]
        │   │       │   │   └── to_out (Linear) weight:[4096, 512]
        │   │       │   │       ├── lora_dropout (ModuleDict)
        │   │       │   │       ├── lora_A (ModuleDict)
        │   │       │   │       │   └── default (Linear) weight:[16, 512]
        │   │       │   │       └── lora_B (ModuleDict)
        │   │       │   │           └── default (Linear) weight:[4096, 16]
        │   │       │   └── ff (Sequential)
        │   │       │       ├── 0 (LayerNorm) weight:[4096] bias:[4096]
        │   │       │       ├── 1 (Linear) weight:[16384, 4096]
        │   │       │       │   ├── lora_dropout (ModuleDict)
        │   │       │       │   ├── lora_A (ModuleDict)
        │   │       │       │   │   └── default (Linear) weight:[16, 4096]
        │   │       │       │   └── lora_B (ModuleDict)
        │   │       │       │       └── default (Linear) weight:[16384, 16]
        │   │       │       └── 3 (Linear) weight:[4096, 16384]
        │   │       │           ├── lora_dropout (ModuleDict)
        │   │       │           ├── lora_A (ModuleDict)
        │   │       │           │   └── default (Linear) weight:[16, 16384]
        │   │       │           └── lora_B (ModuleDict)
        │   │       │               └── default (Linear) weight:[4096, 16]